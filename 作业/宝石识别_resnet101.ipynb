{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data146007  dataset  readme.json\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting beautifulsoup4\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/57/f4/a69c20ee4f660081a7dedb1ac57f29be9378e04edfcb90c526b923d4bebc/beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/49/37/673d6490efc51ec46d198c75903d99de59baffdd47aea3d071b80a9e4e89/soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.2 soupsieve-2.4.1\n",
      "WARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.4.1.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.12.2.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 基于PaddlePaddle2.0-构建卷积网络模型LeNet-5\n",
    "\n",
    "### 1. LeNet-5模型表达式\n",
    "\n",
    "LeNet-5是卷积神经网络模型的早期代表，它由LeCun在1998年提出。该模型采用顺序结构，主要包括7层（2个卷积层、2个池化层和3个全连接层），卷积层和池化层交替排列。以mnist手写数字分类为例构建一个LeNet-5模型,每个手写数字图片样本的宽与高均为28像素，样本标签值是0~9，代表0至9十个数字。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c758063e28754e20ac3ec70cef5ca1b0168ad923000d47f1bd686b59d2f3c23b)\n",
    "\n",
    "图1. 单样本视角的LeNet-5模型原理\n",
    "\n",
    "下面详细解析LeNet-5模型的正向传播过程。\n",
    "\n",
    "（1）卷积层L1\n",
    "\n",
    "单样本视角。L1层的输入数据形状大小为$\\mathbb{R}^{1 \\times 28 \\times 28}$，表示通道数量为1，行与列的大小都为28。输出数据形状大小为$\\mathbb{R}^{6 \\times 24 \\times 24}$，表示通道数量为6，行与列维都为24。\n",
    "\n",
    "批量样本视角。设批量大小为m。L1层的输入数据形状大小为$\\mathbb{R}^{m \\times 1 \\times 28 \\times 28}$，表示样本批量为m，通道数量为1，行与列的大小都为28。L1层的输出数据形状大小为$\\mathbb{R}^{m \\times 6 \\times 24 \\times 24}$，表示样本批量为m，通道数量为6，行与列维都为24。\n",
    "\n",
    "参数视角。L1层的权重形状大小$\\mathbb{R}^{6 \\times 1 \\times 5 \\times 5}$为，偏置项形状大小为6。\n",
    "\n",
    "这里有两个问题很关键：一是，为什么通道数从1变成了6呢？原因是模型的卷积层L1设定了6个卷积核，每个卷积核都与输入数据发生运算，最终分别得到6组数据。二是，为什么行列大小从28变成了24呢？原因是每个卷积核的行维与列维都为5，卷积核（5×5）在输入数据（28×28）上移动，且每次移动步长为1，那么输出数据的行列大小分别为28-5+1=24。\n",
    "\n",
    "（2）池化层L2\n",
    "\n",
    "从单样本视角。L2层的输入数据大小要和L1层的输出数据大小保持一致。输入数据形状大小为$\\mathbb{R}^{6 \\times 24 \\times 24}$，表示通道数量为6，行与列的大小都为24。L2层的输出数据形状大小为$\\mathbb{R}^{6 \\times 12 \\times 12}$，表示通道数量为6，行与列维都为12。\n",
    "\n",
    "从批量样本视角。设批量大小为m。L2层的输入数据形状大小为$\\mathbb{R}^{m \\times 6 \\times 24 \\times 24}$，表示样本批量为m，通道数量为6，行与列的大小都为24。L2层的输出数据形状大小为$\\mathbb{R}^{m \\times 6 \\times 12 \\times 12}$，表示样本批量为m，通道数量为6，行与列维都为12。为什么行列大小从24变成了12呢？原因是池化层中的过滤器形状大小为2×2，其在输入数据（24×24）上移动，且每次移动步长（跨距）为2，每次选择4个数（2×2）中最大值作为输出，那么输出数据的行列大小分别为24÷2=12。\n",
    "\n",
    "（3）卷积层L3\n",
    "\n",
    "单样本视角。L3层的输入数据形状大小为$\\mathbb{R}^{6 \\times 12 \\times 12}$，表示通道数量为6，行与列的大小都为12。L3层的输出数据形状大小为$\\mathbb{R}^{6 \\times 8 \\times 8}$，表示通道数量为16，行与列维都为8。\n",
    "\n",
    "批量样本视角。设批量大小为m。L3层的输入数据形状大小为$\\mathbb{R}^{m \\times 6 \\times 12 \\times 12}$，表示样本批量为m，通道数量为6，行与列的大小都为12。L3层的输出数据形状大小为$\\mathbb{R}^{m \\times 16 \\times 8 \\times 8}$，表示样本批量为m，通道数量为16，行与列维都为8。\n",
    "\n",
    "参数视角。L3层的权重形状大小为$\\mathbb{R}^{m \\times 16 \\times 6 \\times 5 \\times 5}$，偏置项形状大小为16。\n",
    "\n",
    "（4）池化层L4\n",
    "\n",
    "从单样本视角。L4层的输入数据形状大小与L3层的输出数据大小一致。L4层的输入数据形状大小为$\\mathbb{R}^{16 \\times 8 \\times 8}$，表示通道数量为16，行与列的大小都为8。L4层的输出数据形状大小为$\\mathbb{R}^{16 \\times 4 \\times 4}$，表示通道数量为16，行与列维都为4。\n",
    "\n",
    "从批量样本视角。设批量大小为m。L4层的输入数据形状大小为$\\mathbb{R}^{m \\times 16 \\times 8 \\times 8}$，表示样本批量为m，通道数量为16，行与列的大小都为8。L4层的输出数据形状大小为$\\mathbb{R}^{m \\times 16 \\times 4 \\times 4}$，表示样本批量为m，通道数量为16，行与列维都为4。池化层L4中的过滤器形状大小为2×2，其在输入数据（形状大小24×24）上移动，且每次移动步长（跨距）为2，每次选择4个数（形状大小2×2）中最大值作为输出。\n",
    "\n",
    "（5）线性层L5\n",
    "\n",
    "从单样本视角。由于L5层是线性层，其输入大小为一维，所以需要把L4层的输出数据大小进行重新划分。L4层的输出形状大小为$\\mathbb{R}^{16 \\times 4 \\times 4}$，则L5层的一维输入形状大小为16×4×4=256。L4层的一维输出大小为120。\n",
    "\n",
    "从批量样本视角。设批量大小为m。L5层输入数据形状大小为$\\mathbb{R}^{m \\times 256}$，表示样本批量为m，输入特征数量为256。输出数据形状大小为$\\mathbb{R}^{m \\times 120}$，表示样本批量为m，输出特征数量为120。\n",
    "\n",
    "（6）线性层L6\n",
    "\n",
    "从单样本视角。L6层的输入特征数量为120。L6层的输出特征数量为84。\n",
    "\n",
    "从批量样本视角。设批量大小为m。L6层的输入数据形状大小为$\\mathbb{R}^{m \\times 120}$，表示样本批量为m，输入特征数量为120。L6层的输出数据形状大小为$\\mathbb{R}^{m \\times 84}$，表示样本批量为m，输出特征数量为84。\n",
    "\n",
    "（7）线性层L7\n",
    "\n",
    "从单样本视角。L7层的输入特征数量为84。L7层的输出特征数量为10。\n",
    "\n",
    "从批量样本视角。设批量大小为m。L7层的输入数据形状大小为$\\mathbb{R}^{m \\times 84}$，表示样本批量为m，输入特征数量为84。L7层的输出数据形状大小为$\\mathbb{R}^{m \\times 10}$，表示样本批量为m，输出特征数量为10。\n",
    "\n",
    "由于是分类问题，我们选择交叉熵损失函数。交叉熵主要用于衡量估计值与真实值之间的差距。交叉熵值越小，模型预测效果越好。\n",
    "\n",
    "$E(\\mathbf{y}^{i},\\mathbf{\\hat{y}}^{i})=-\\sum_{j=1}^{q}\\mathbf{y}_{j}^{i}ln(\\mathbf{\\hat{y}}_{j}^{i})$\n",
    "\n",
    "其中，$\\mathbf{y}^{i} \\in \\mathbb{R}^{q}$为真实值，$y_{j}^{i}$是$\\mathbf{y}^{i}$中的元素(取值为0或1)，$j=1,...,q$。$\\mathbf{\\hat{y}^{i}} \\in \\mathbb{R}^{q}$是预测值（样本在每个类别上的概率）。\n",
    "\n",
    "定义好了正向传播过程之后，接着随机化初始参数，然后便可以计算出每层的结果，每次将得到m×10的矩阵作为预测结果，其中m是小批量样本数。接下来进行反向传播过程，预测结果与真实结果之间肯定存在差异，以缩减该差异作为目标，计算模型参数梯度。进行多轮迭代，便可以优化模型，使得预测结果与真实结果之间更加接近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "作业要求：\n",
    "\n",
    "参考lenet模型，构建一个用于宝石识别的cnn模型，训练模型力求精确度越高越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data146007  dataset  readme.json\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\r\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \r\n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#导入需要的包\r\n",
    "import os\r\n",
    "import zipfile\r\n",
    "import random\r\n",
    "import json\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "from PIL import Image\r\n",
    "import paddle\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from paddle.io import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\r\n",
    "参数配置\r\n",
    "'''\r\n",
    "train_parameters = {\r\n",
    "    \"input_size\": [16,224,224],                           #输入图片的shape\r\n",
    "    \"class_dim\": -1,                                     #分类数\r\n",
    "    \"src_path\":\"data/data146007/archive_train.zip\",       #原始数据集路径\r\n",
    "    \"target_path\":\"/home/aistudio/data/dataset\",        #要解压的路径 \r\n",
    "    \"train_list_path\": \"./train.txt\",              #train_data.txt路径\r\n",
    "    \"eval_list_path\": \"./eval.txt\",                  #eval_data.txt路径\r\n",
    "    \"label_list_path\": \"./label.txt\",                  #eval_data.txt路径\r\n",
    "    \"label_dict\":{},                                    #标签字典\r\n",
    "    \"readme_path\": \"/home/aistudio/data/readme.json\",   #readme.json路径\r\n",
    "    \"num_epochs\": 40,                                    #训练轮数\r\n",
    "    \"train_batch_size\": 16,                             #批次的大小\r\n",
    "    \"learning_strategy\": {                              #优化函数相关的配置\r\n",
    "        \"lr\": 0.001                                     #超参数学习率\r\n",
    "    } \r\n",
    "}\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "print(train_parameters['learning_strategy']['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unzip_data(src_path,target_path):\r\n",
    "\r\n",
    "    '''\r\n",
    "    解压原始数据集，将src_path路径下的zip包解压至data/dataset目录下\r\n",
    "    '''\r\n",
    "\r\n",
    "    if(not os.path.isdir(target_path)):    \r\n",
    "        z = zipfile.ZipFile(src_path, 'r')\r\n",
    "        z.extractall(path=target_path)\r\n",
    "        z.close()\r\n",
    "    else:\r\n",
    "        print(\"文件已解压\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_list(target_path,train_list_path,eval_list_path,label_list_path):\r\n",
    "    '''\r\n",
    "    生成数据列表\r\n",
    "    '''\r\n",
    "    #存放所有类别的信息\r\n",
    "    class_detail = []\r\n",
    "    #获取所有类别保存的文件夹名称\r\n",
    "    data_list_path=target_path\r\n",
    "    class_dirs = os.listdir(data_list_path)\r\n",
    "    if '__MACOSX' in class_dirs:\r\n",
    "        class_dirs.remove('__MACOSX')\r\n",
    "    # #总的图像数量\r\n",
    "    all_class_images = 0\r\n",
    "    # #存放类别标签\r\n",
    "    class_label=0\r\n",
    "    # #存放类别数目\r\n",
    "    class_dim = 0\r\n",
    "    # #存储要写进eval.txt和train.txt中的内容\r\n",
    "    trainer_list=[]\r\n",
    "    eval_list=[]\r\n",
    "    label_list=[]\r\n",
    "    #读取每个类别\r\n",
    "    for class_dir in class_dirs:\r\n",
    "        if class_dir != \".DS_Store\":\r\n",
    "            if ' ' in class_dir:    #若文件夹名中存在空格\r\n",
    "                d=class_dir.split(' ')    #以空格作为分隔符\r\n",
    "                new_name1=os.path.join(d[0]+\"_\"+d[1])\r\n",
    "                os.rename(os.path.join(data_list_path,class_dir),os.path.join(data_list_path,new_name1))  #重命名 \r\n",
    "                class_dir=new_name1           \r\n",
    "            class_dim += 1\r\n",
    "            #每个类别的信息\r\n",
    "            class_detail_list = {}\r\n",
    "            eval_sum = 0\r\n",
    "            trainer_sum = 0\r\n",
    "            #统计每个类别有多少张图片\r\n",
    "            class_sum = 0\r\n",
    "            #获取类别路径 \r\n",
    "            path = os.path.join(data_list_path,class_dir)\r\n",
    "            # 获取所有图片\r\n",
    "            img_paths = os.listdir(path)\r\n",
    "            for img_path in img_paths:                                  # 遍历文件夹下的每个图片\r\n",
    "                if img_path =='.DS_Store':\r\n",
    "                    continue\r\n",
    "                if ' ' in img_path:    #若文件夹名中存在空格\r\n",
    "                    d=img_path.split(' ')    #以空格作为分隔符\r\n",
    "                    new_name2=os.path.join(d[0]+\"_\"+d[1])\r\n",
    "                    os.rename(os.path.join(path,img_path),os.path.join(path,new_name2))  #重命名\r\n",
    "                    img_path=new_name2\r\n",
    "                name_path = os.path.join(path,img_path)                       # 每张图片的路径\r\n",
    "                if class_sum % 15 == 0:                                 # 每10张图片取一个做验证数据\r\n",
    "                    eval_sum += 1                                       # eval_sum为测试数据的数目\r\n",
    "                    eval_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")\r\n",
    "                else:\r\n",
    "                    trainer_sum += 1 \r\n",
    "                    trainer_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")#trainer_sum测试数据的数目\r\n",
    "                class_sum += 1                                          #每类图片的数目\r\n",
    "                all_class_images += 1                                   #所有类图片的数目\r\n",
    "            \r\n",
    "            # 说明的json文件的class_detail数据\r\n",
    "            class_detail_list['class_name'] = class_dir             #类别名称\r\n",
    "            class_detail_list['class_label'] = class_label          #类别标签\r\n",
    "            class_detail_list['class_eval_images'] = eval_sum       #该类数据的测试集数目\r\n",
    "            class_detail_list['class_trainer_images'] = trainer_sum #该类数据的训练集数目\r\n",
    "            class_detail.append(class_detail_list)  \r\n",
    "            #初始化标签列表\r\n",
    "            train_parameters['label_dict'][str(class_label)] = class_dir\r\n",
    "            #label_dict字典中，类别标签为class_\r\n",
    "            class_label += 1\r\n",
    "            label_list.append(class_dir+\"\\n\")\r\n",
    "            \r\n",
    "    #初始化分类数\r\n",
    "    train_parameters['class_dim'] = class_dim\r\n",
    "    print(train_parameters)\r\n",
    "    #乱序  \r\n",
    "    random.shuffle(eval_list)\r\n",
    "    with open(eval_list_path, 'a') as f:\r\n",
    "        for eval_image in eval_list:\r\n",
    "            f.write(eval_image) \r\n",
    "    #乱序        \r\n",
    "    random.shuffle(trainer_list) \r\n",
    "    with open(train_list_path, 'a') as f2:\r\n",
    "        for train_image in trainer_list:\r\n",
    "            f2.write(train_image) \r\n",
    "    #乱序        \r\n",
    "    random.shuffle(label_list) \r\n",
    "    with open(label_list_path, 'a') as f2:\r\n",
    "        for image_label in label_list:\r\n",
    "            f2.write(image_label) \r\n",
    "    # 说明的json文件信息\r\n",
    "    readjson = {}\r\n",
    "    readjson['all_class_name'] = data_list_path                  #文件父目录\r\n",
    "    readjson['all_class_images'] = all_class_images\r\n",
    "    readjson['class_detail'] = class_detail\r\n",
    "    jsons = json.dumps(readjson, sort_keys=True, indent=4, separators=(',', ': '))\r\n",
    "    with open(train_parameters['readme_path'],'w') as f:\r\n",
    "        f.write(jsons)\r\n",
    "    print ('生成数据列表完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已解压\n",
      "{'input_size': [16, 224, 224], 'class_dim': 25, 'src_path': 'data/data146007/archive_train.zip', 'target_path': '/home/aistudio/data/dataset', 'train_list_path': './train.txt', 'eval_list_path': './eval.txt', 'label_list_path': './label.txt', 'label_dict': {'0': 'Iolite', '1': 'Malachite', '2': 'Danburite', '3': 'Hessonite', '4': 'Alexandrite', '5': 'Fluorite', '6': 'Almandine', '7': 'Cats_Eye', '8': 'Diamond', '9': 'Emerald', '10': 'Quartz_Beer', '11': 'Tanzanite', '12': 'Benitoite', '13': 'Onyx_Black', '14': 'Garnet_Red', '15': 'Jade', '16': 'Variscite', '17': 'Zircon', '18': 'Kunzite', '19': 'Pearl', '20': 'Sapphire_Blue', '21': 'Beryl_Golden', '22': 'Labradorite', '23': 'Carnelian', '24': 'Rhodochrosite'}, 'readme_path': '/home/aistudio/data/readme.json', 'num_epochs': 40, 'train_batch_size': 16, 'learning_strategy': {'lr': 0.001}}\n",
      "生成数据列表完成！\n"
     ]
    }
   ],
   "source": [
    "'''\r\n",
    "参数初始化\r\n",
    "'''\r\n",
    "src_path=train_parameters['src_path']\r\n",
    "target_path=train_parameters['target_path']\r\n",
    "train_list_path=train_parameters['train_list_path']\r\n",
    "eval_list_path=train_parameters['eval_list_path']\r\n",
    "label_list_path=train_parameters['label_list_path']\r\n",
    "batch_size=train_parameters['train_batch_size']\r\n",
    "'''\r\n",
    "解压原始数据到指定路径\r\n",
    "'''\r\n",
    "unzip_data(src_path,target_path)\r\n",
    "\r\n",
    "'''\r\n",
    "划分训练集与验证集，乱序，生成数据列表\r\n",
    "'''\r\n",
    "#每次生成数据列表前，首先清空train.txt和eval.txt\r\n",
    "with open(train_list_path, 'w') as f: \r\n",
    "    f.seek(0)\r\n",
    "    f.truncate() \r\n",
    "with open(eval_list_path, 'w') as f: \r\n",
    "    f.seek(0)\r\n",
    "    f.truncate() \r\n",
    "    \r\n",
    "#生成数据列表   \r\n",
    "get_data_list(target_path,train_list_path,eval_list_path,label_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddle.vision import transforms as T\r\n",
    "\r\n",
    "class Reader(Dataset):\r\n",
    "    def __init__(self, data_path, mode='train'):\r\n",
    "        \"\"\"\r\n",
    "        数据读取器\r\n",
    "        :param data_path: 数据集所在路径\r\n",
    "        :param mode: train or eval\r\n",
    "        \"\"\"\r\n",
    "        super().__init__()\r\n",
    "        self.data_path = data_path\r\n",
    "        self.img_paths = []\r\n",
    "        self.labels = []\r\n",
    "        #数据增强\r\n",
    "        self.transform =T.Compose([T.Resize((256, 256)), \r\n",
    "                        T.CenterCrop(224), \r\n",
    "                        T.RandomHorizontalFlip(0.5),\r\n",
    "                        T.RandomRotation(15),\r\n",
    "                        T.Transpose(),\r\n",
    "                        T.Normalize(mean=255, data_format='CHW', to_rgb=True),\r\n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\r\n",
    "        if mode == 'train':\r\n",
    "            with open(os.path.join(self.data_path, \"train.txt\"), \"r\", encoding=\"utf-8\") as f:\r\n",
    "                self.info = f.readlines()\r\n",
    "            for img_info in self.info:\r\n",
    "                img_path, label = img_info.strip().split('\\t')\r\n",
    "                self.img_paths.append(img_path)\r\n",
    "                self.labels.append(int(label))\r\n",
    "\r\n",
    "        else:\r\n",
    "            with open(os.path.join(self.data_path, \"eval.txt\"), \"r\", encoding=\"utf-8\") as f:\r\n",
    "                self.info = f.readlines()\r\n",
    "            for img_info in self.info:\r\n",
    "                img_path, label = img_info.strip().split('\\t')\r\n",
    "                self.img_paths.append(img_path)\r\n",
    "                self.labels.append(int(label))\r\n",
    "\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        \"\"\"\r\n",
    "        获取一组数据\r\n",
    "        :param index: 文件索引号\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        # 第一步打开图像文件并获取label值\r\n",
    "        img_path = self.img_paths[index]\r\n",
    "        img = Image.open(img_path)\r\n",
    "        if img.mode != 'RGB':\r\n",
    "            img = img.convert('RGB') \r\n",
    "        img = img.resize((224, 224), Image.BILINEAR)\r\n",
    "        img = np.array(img).astype('float32')\r\n",
    "        img = img.transpose((2, 0, 1)) / 255\r\n",
    "        label = self.labels[index]\r\n",
    "        label = np.array([label], dtype=\"int64\")\r\n",
    "        return img, label\r\n",
    "\r\n",
    "    def print_sample(self, index: int = 0):\r\n",
    "        print(\"文件名\", self.img_paths[index], \"\\t标签值\", self.labels[index])\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = Reader('/home/aistudio/',mode='train')\r\n",
    "\r\n",
    "eval_dataset = Reader('/home/aistudio/',mode='eval')\r\n",
    "#训练数据加载\r\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=16, shuffle=True)\r\n",
    "#测试数据加载\r\n",
    "eval_loader = paddle.io.DataLoader(eval_dataset, batch_size = 8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件名 /home/aistudio/data/dataset/Labradorite/labradorite_35.jpg \t标签值 22\n",
      "742\n",
      "文件名 /home/aistudio/data/dataset/Malachite/malachite_18.jpg \t标签值 1\n",
      "69\n",
      "(3, 224, 224)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "train_dataset.print_sample(200)\r\n",
    "print(train_dataset.__len__())\r\n",
    "eval_dataset.print_sample(0)\r\n",
    "print(eval_dataset.__len__())\r\n",
    "print(eval_dataset.__getitem__(10)[0].shape)\r\n",
    "print(eval_dataset.__getitem__(10)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: paddlex==2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (4.1.1.26)\n",
      "Requirement already satisfied: scikit-learn==0.23.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (0.23.2)\n",
      "Requirement already satisfied: visualdl>=2.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (2.4.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (1.6.3)\n",
      "Requirement already satisfied: shapely>=1.7.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: paddleslim==2.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (2.1.1)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (0.4.4)\n",
      "Requirement already satisfied: pycocotools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (4.27.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (5.1.2)\n",
      "Requirement already satisfied: motmetrics in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (1.4.0)\n",
      "Requirement already satisfied: chardet in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: lap in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlex==2.0.0) (0.4.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddleslim==2.1.1->paddlex==2.0.0) (2.2.3)\n",
      "Requirement already satisfied: pyzmq in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddleslim==2.1.1->paddlex==2.0.0) (23.2.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddleslim==2.1.1->paddlex==2.0.0) (8.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn==0.23.2->paddlex==2.0.0) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn==0.23.2->paddlex==2.0.0) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn==0.23.2->paddlex==2.0.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.1.1->paddlex==2.0.0) (3.20.0)\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.1.1->paddlex==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.1.1->paddlex==2.0.0) (1.1.5)\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.1.1->paddlex==2.0.0) (1.1.1)\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.1.1->paddlex==2.0.0) (0.8.53)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.1.1->paddlex==2.0.0) (2.24.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.1.1->paddlex==2.0.0) (1.16.0)\n",
      "Requirement already satisfied: xmltodict>=0.12.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from motmetrics->paddlex==2.0.0) (0.13.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.1.1->paddlex==2.0.0) (0.16.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.1.1->paddlex==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.1.1->paddlex==2.0.0) (7.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.1.1->paddlex==2.0.0) (3.0.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.1.1->paddlex==2.0.0) (2019.3)\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.1.1->paddlex==2.0.0) (2.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddleslim==2.1.1->paddlex==2.0.0) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddleslim==2.1.1->paddlex==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddleslim==2.1.1->paddlex==2.0.0) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddleslim==2.1.1->paddlex==2.0.0) (3.0.9)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl>=2.1.1->paddlex==2.0.0) (3.9.9)\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl>=2.1.1->paddlex==2.0.0) (0.18.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.1.1->paddlex==2.0.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.1.1->paddlex==2.0.0) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.1.1->paddlex==2.0.0) (1.25.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl>=2.1.1->paddlex==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->paddleslim==2.1.1->paddlex==2.0.0) (56.2.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install paddlex==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlex as pdx\r\n",
    "from paddlex import transforms as T\r\n",
    "train_transforms = T.Compose(\r\n",
    "    [T.RandomCrop(crop_size=224), T.RandomHorizontalFlip(), T.Normalize()])\r\n",
    "\r\n",
    "eval_transforms = T.Compose([\r\n",
    "    T.ResizeByShort(short_size=256), T.CenterCrop(crop_size=224), T.Normalize()\r\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-16 15:57:51 [INFO]\tStarting to read file list from dataset...\n",
      "2023-05-16 15:57:51 [INFO]\t742 samples in file /home/aistudio/train.txt\n",
      "2023-05-16 15:57:51 [INFO]\tStarting to read file list from dataset...\n",
      "2023-05-16 15:57:51 [INFO]\t69 samples in file /home/aistudio/eval.txt\n",
      "2023-05-16 15:57:52 [INFO]\tLoading pretrained model from output/resNet101_vd_ssld/pretrain/ResNet101_vd_ssld_pretrained.pdparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-62:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\", line 620, in _get_data\n",
      "    data = self._data_queue.get(timeout=self._timeout)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/multiprocessing/queues.py\", line 105, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\", line 534, in _thread_loop\n",
      "    batch = self._get_data()\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\", line 636, in _get_data\n",
      "    \"pids: {}\".format(len(failed_workers), pids))\n",
      "RuntimeError: DataLoader 2 workers exit unexpectedly, pids: 4404, 4405\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-16 15:57:54 [WARNING]\t[SKIP] Shape of pretrained params out.weight doesn't match.(Pretrained: [2048, 1000], Actual: [2048, 50])\n",
      "2023-05-16 15:57:54 [WARNING]\t[SKIP] Shape of pretrained params out.bias doesn't match.(Pretrained: [1000], Actual: [50])\n",
      "2023-05-16 15:57:54 [INFO]\tThere are 530/532 variables loaded into ResNet101_vd_ssld.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pdx.datasets.ImageNet(\r\n",
    "    data_dir='/home/aistudio/data/dataset',\r\n",
    "    file_list='/home/aistudio/train.txt',\r\n",
    "    label_list='/home/aistudio/label.txt',\r\n",
    "    transforms=train_transforms,\r\n",
    "    shuffle=True)\r\n",
    "    \r\n",
    "eval_dataset = pdx.datasets.ImageNet(\r\n",
    "    data_dir='/home/aistudio/data/dataset',\r\n",
    "    file_list='/home/aistudio/eval.txt',\r\n",
    "    label_list='/home/aistudio/label.txt',\r\n",
    "    transforms=eval_transforms)\r\n",
    "\r\n",
    "num_classes = len(train_dataset.labels)\r\n",
    "model = pdx.cls.ResNet101_vd_ssld(num_classes=num_classes)\r\n",
    "model.train(num_epochs=20,\r\n",
    "            train_dataset=train_dataset,\r\n",
    "            train_batch_size=16,\r\n",
    "            eval_dataset=eval_dataset,\r\n",
    "            lr_decay_epochs=[6, 8],\r\n",
    "            save_interval_epochs=1,\r\n",
    "            learning_rate=0.005,\r\n",
    "            save_dir='output/resNet101_vd_ssld',\r\n",
    "            use_vdl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Batch=0\r\n",
    "# Batchs=[]\r\n",
    "# all_train_accs=[]\r\n",
    "# def draw_train_acc(Batchs, train_accs):\r\n",
    "#     title=\"training accs\"\r\n",
    "#     plt.title(title, fontsize=24)\r\n",
    "#     plt.xlabel(\"batch\", fontsize=14)\r\n",
    "#     plt.ylabel(\"acc\", fontsize=14)\r\n",
    "#     plt.plot(Batchs, train_accs, color='green', label='training accs')\r\n",
    "#     plt.legend()\r\n",
    "#     plt.grid()\r\n",
    "#     plt.show()\r\n",
    "\r\n",
    "# all_train_loss=[]\r\n",
    "# def draw_train_loss(Batchs, train_loss):\r\n",
    "#     title=\"training loss\"\r\n",
    "#     plt.title(title, fontsize=24)\r\n",
    "#     plt.xlabel(\"batch\", fontsize=14)\r\n",
    "#     plt.ylabel(\"loss\", fontsize=14)\r\n",
    "#     plt.plot(Batchs, train_loss, color='red', label='training loss')\r\n",
    "#     plt.legend()\r\n",
    "#     plt.grid()\r\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. 定义模型\n",
    "\n",
    "在下方cell中自定义CNN(lenet)网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import paddle\r\n",
    "# from paddle.nn import Conv2D, MaxPool2D, Linear,BatchNorm2D\r\n",
    "# import paddle.nn.functional as F\r\n",
    "# class MyCNN(paddle.nn.Layer):  #@save\r\n",
    "#     def __init__(self):\r\n",
    "#         super(MyCNN, self).__init__()\r\n",
    "#         self.conv1=Conv2D(in_channels=3, out_channels=32, kernel_size=3,stride=1,padding=2)\r\n",
    "#         self.pool1=MaxPool2D(kernel_size=2,stride=2)\r\n",
    "#         self.conv2=Conv2D(in_channels=32,out_channels=32,kernel_size=3,stride=1,padding=2)\r\n",
    "#         self.pool2=MaxPool2D(kernel_size=2,stride=2)\r\n",
    "#         # self.conv3=Conv2D(in_channels=32,out_channels=32,kernel_size=3,stride=1,padding=2)\r\n",
    "#         # self.pool3=MaxPool2D(kernel_size=2,stride=2)\r\n",
    "#         # if use_1x1conv:\r\n",
    "#         #     self.conv3 = Conv2D(input_channels, num_channels,\r\n",
    "#         #                            kernel_size=1, stride=strides)\r\n",
    "#         # else:\r\n",
    "#         #     self.conv3 = None\r\n",
    "#         self.bn1 = BatchNorm2D(32)#out_channels\r\n",
    "#         self.bn2 = BatchNorm2D(32)\r\n",
    "#         self.fc1 = paddle.fluid.dygraph.Linear(input_dim=32*57*57, output_dim=1024, act='relu')\r\n",
    "#         self.fc2 = paddle.fluid.dygraph.Linear(input_dim=1024, output_dim=512, act='relu')\r\n",
    "#         self.fc3 = paddle.fluid.dygraph.Linear(input_dim=512, output_dim=128, act='relu')\r\n",
    "#         self.fc4 = paddle.fluid.dygraph.Linear(input_dim=128, output_dim=64, act='relu')\r\n",
    "#     def forward(self, input):\r\n",
    "#         Y = F.relu(self.bn1(self.conv1(input)))\r\n",
    "#         Y=self.pool1(Y)\r\n",
    "#         Y = self.bn2(self.conv2(Y))\r\n",
    "#         Y=self.pool2(Y)\r\n",
    "#         # Y=self.bn2(self.conv3(Y))\r\n",
    "#         # Y=self.pool3(Y)\r\n",
    "\r\n",
    "#         X=F.relu(self.bn1(self.conv1(input)))\r\n",
    "#         X=self.pool1(X)\r\n",
    "#         X = self.bn2(self.conv2(X))\r\n",
    "#         X=self.pool2(X)\r\n",
    "#         # X=self.conv3(X)\r\n",
    "#         # X=self.pool3(X)\r\n",
    "#         # if self.conv3:\r\n",
    "#         #     X = self.conv3(X)\r\n",
    "#         # print(Y[0].shape)\r\n",
    "\r\n",
    "#         Y+=X\r\n",
    "#         Y=paddle.reshape(Y,shape=[-1,32*57*57])\r\n",
    "#         Y=self.fc1(Y)\r\n",
    "#         Y=self.fc2(Y)\r\n",
    "#         Y=self.fc3(Y)\r\n",
    "#         Y=self.fc4(Y)\r\n",
    "#         return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model=MyCNN() #模型实例化\r\n",
    "# model.train() #训练模式\r\n",
    "# cross_entropy = paddle.nn.CrossEntropyLoss()  \r\n",
    "# opt=paddle.optimizer.SGD(learning_rate=train_parameters['learning_strategy']['lr'], parameters=model.parameters())  #优化器设定，使用随机梯度下降算法的优化器\r\n",
    "\r\n",
    "# epochs_num=train_parameters['num_epochs'] #迭代次数\r\n",
    "# for pass_num in range(epochs_num):\r\n",
    "#     for batch_id,data in enumerate(train_loader()):\r\n",
    "#         image = data[0]\r\n",
    "#         label = data[1]\r\n",
    "\r\n",
    "#         predict=model(image) #数据传入model\r\n",
    "\r\n",
    "#         loss=cross_entropy(predict,label)\r\n",
    "#         acc=paddle.metric.accuracy(predict,label)#计算精度\r\n",
    "        \r\n",
    "#         if batch_id!=0 and batch_id%5==0:\r\n",
    "#             Batch = Batch+5 \r\n",
    "#             Batchs.append(Batch)\r\n",
    "#             all_train_loss.append(loss.numpy()[0])\r\n",
    "#             all_train_accs.append(acc.numpy()[0])\r\n",
    "            \r\n",
    "#             print(\"train_pass:{},batch_id:{},train_loss:{},train_acc:{}\".format(pass_num,batch_id,loss.numpy(),acc.numpy()))\r\n",
    "        \r\n",
    "#         loss.backward() #进行反向传播求出梯度      \r\n",
    "#         opt.step()  #执行一次优化器并进行参数更新。\r\n",
    "#         opt.clear_grad()   #opt.clear_grad()来重置梯度\r\n",
    "\r\n",
    "# paddle.save(model.state_dict(),'MyCNN')#保存模型\r\n",
    "\r\n",
    "# draw_train_acc(Batchs,all_train_accs)\r\n",
    "# draw_train_loss(Batchs,all_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #模型评估\r\n",
    "# para_state_dict = paddle.load(\"MyCNN\")\r\n",
    "# model = MyCNN()\r\n",
    "# model.set_state_dict(para_state_dict) #加载模型参数\r\n",
    "# model.eval() #验证模式\r\n",
    "\r\n",
    "# accs = []\r\n",
    "\r\n",
    "# for batch_id,data in enumerate(eval_loader()):#测试集\r\n",
    "#     image=data[0]\r\n",
    "#     label=data[1]     \r\n",
    "#     predict=model(image)        \r\n",
    "#     acc=paddle.metric.accuracy(predict,label)\r\n",
    "#     accs.append(acc.numpy()[0])\r\n",
    "#     avg_acc = np.mean(accs)\r\n",
    "# print(\"当前模型在测试集上的准确率为:\",avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\r\n",
    "# import zipfile\r\n",
    "\r\n",
    "# def unzip_infer_data(src_path,target_path):\r\n",
    "#     '''\r\n",
    "#     解压预测数据集\r\n",
    "#     '''\r\n",
    "#     if(not os.path.isdir(target_path)):     \r\n",
    "#         z = zipfile.ZipFile(src_path, 'r')\r\n",
    "#         z.extractall(path=target_path)\r\n",
    "#         z.close()\r\n",
    "\r\n",
    "\r\n",
    "# def load_image(img_path):\r\n",
    "#     '''\r\n",
    "#     预测图片预处理\r\n",
    "#     '''\r\n",
    "#     img = Image.open(img_path) \r\n",
    "#     if img.mode != 'RGB': \r\n",
    "#         img = img.convert('RGB') \r\n",
    "#     img = img.resize((224, 224), Image.BILINEAR)\r\n",
    "#     img = np.array(img).astype('float32') \r\n",
    "#     img = img.transpose((2, 0, 1))  # HWC to CHW \r\n",
    "#     img = img/255                # 像素值归一化 \r\n",
    "#     return img\r\n",
    "\r\n",
    "\r\n",
    "# infer_src_path = '/home/aistudio/data/data146007/archive_test.zip'\r\n",
    "# infer_dst_path = '/home/aistudio/data/archive_test'\r\n",
    "# unzip_infer_data(infer_src_path,infer_dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\r\n",
    "模型预测\r\n",
    "'''\r\n",
    "para_state_dict = paddle.load(\"MyCNN\")\r\n",
    "model = MyCNN()\r\n",
    "model.set_state_dict(para_state_dict) #加载模型参数\r\n",
    "model.eval() #训练模式\r\n",
    "\r\n",
    "#展示预测图片\r\n",
    "infer_path='data/archive_test/alexandrite_3.jpg'\r\n",
    "img = Image.open(infer_path)\r\n",
    "plt.imshow(img)          #根据数组绘制图像\r\n",
    "plt.show()               #显示图像\r\n",
    "\r\n",
    "#对预测图片进行预处理\r\n",
    "infer_imgs = []\r\n",
    "infer_imgs.append(load_image(infer_path))\r\n",
    "infer_imgs = np.array(infer_imgs)\r\n",
    "\r\n",
    "label_dic = train_parameters['label_dict']\r\n",
    "print(label_dic)\r\n",
    "for i in range(len(infer_imgs)):\r\n",
    "    data = infer_imgs[i]\r\n",
    "    dy_x_data = np.array(data).astype('float32')\r\n",
    "    dy_x_data=dy_x_data[np.newaxis,:, : ,:]\r\n",
    "    img = paddle.to_tensor (dy_x_data)\r\n",
    "    out = model(img)\r\n",
    "    lab = np.argmax(out.numpy())  #argmax():返回最大数的索引\r\n",
    "\r\n",
    "    print(\"第{}个样本,被预测为：{},真实标签为：{}\".format(i+1,label_dic[str(lab)],infer_path.split('/')[-1].split(\"_\")[0]))\r\n",
    "        \r\n",
    "print(\"结束\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
