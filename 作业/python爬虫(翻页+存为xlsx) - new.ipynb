{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python爬虫"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本流程\n",
    "* **准备工作**\n",
    "  \n",
    "  通过浏览器查看要分析的目标网页\n",
    "* **获取数据**\n",
    "  \n",
    "  通过http库向目标站点发起请求，请求可以包含额外的header等信息。如果服务器能正常响应，会得到一个response，即为所要获取的页面内容\n",
    "* **解析内容**\n",
    "  \n",
    "  得到的内容可能是HTML、json等格式，可以用页面解析库、正则表达式等进行解析。\n",
    "* **保存数据**\n",
    "  \n",
    "  保存形式多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、准备工作\n",
    "首先安装lxml、beautifulsoup包\n",
    "win+r打开cmd窗口\n",
    "输入\n",
    "```\n",
    "pip install beautifulsoup4\n",
    "pip install lxml \n",
    "```\n",
    "打开[豆瓣阅读网站](https://read.douban.com/provider/all)，用F12查看网站源码。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、获取数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **request.get()** 发送http get请求，返回响应操作\n",
    "  * 需要 `import requests`库\n",
    "  * response对象常用方法：\n",
    "    ```\n",
    "    r.headers\n",
    "    #http响应内容的头部内容，来返回get请求获得网页的头部信息。\n",
    "    r.status_code\n",
    "    #http请求的返回状态，200表示连接成功，404表示连接失败\n",
    "    r.text\n",
    "    #http响应内容的字符串形式，url对应的页面内容\n",
    "    r.encoding\n",
    "    #从HTTP header中猜测的响应内容编码方式\n",
    "    r.apparent_encoding\n",
    "    #从内容分析出的响应内容的编码方式（备选编码方式）\n",
    "    r.content\n",
    "    #HTTP响应内容的二进制形式\n",
    "    ```\n",
    "\n",
    "* 用**BeautifulSoup**从HTML/XML中提取数据\n",
    "  \n",
    "  * BeautifulSoup(markup, \"html.parser\")或BeautifulSoup(markup, \"lxml\")，推荐使用lxml作为解析器,因为效率更高。\n",
    "  *  find_next()：用于查找符合查询条件的第一 个标签节点。\n",
    "  *  find_all():查找所有符合查询条件的标签节点，并返回一个列表。\n",
    "  *  \n",
    "     ```\n",
    "     //将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象, 可以传入一段字符串\n",
    "     sp = BeautifulSoup(response.text,'lxml') \n",
    "      ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **headers和cookies的关系**\n",
    "\n",
    "  怎么说呢，网站是欢迎真人访问的，这会提高网站的流量和热度等等，但是网站不欢迎机器人，机器人做的事情对于网站\n",
    "  来说是无利可图的。大家应该已经猜到了，这个机器人就是我们的爬虫，而这个借用法官的名声就是headers；后面的那些法官凭证就是网站为了安全设置的cookies。\n",
    "* **如何通过浏览器获取网页的cookie**\n",
    "  * 打开要爬取的网页\n",
    "  * 打开开发者模式，按F12\n",
    "  * 点击左侧框的all文件\n",
    "  * 在上方元素/控制../网络/ 中找到网络一栏\n",
    "  * 在右侧找到cookie并复制\n",
    "* **find()和find_next()辨析**\n",
    "  \n",
    "  * find()和find_next()都只搜索传入区域内的板块\n",
    "  * find()返回传入区域要搜索的第一个位置。find_next()返回传入区域的下一个位置\n",
    "    eg.当传入区域li_s含多个div时，li_s.find('div')永远返回第一个。用li_s.find_next('div').find_next('div')可以找到第二个div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='read.douban.com', port=443): Max retries exceeded with url: /provider/all (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000019798EB5400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\socket.py:918\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 918\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[0;32m    919\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\connectionpool.py:711\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[39mif\u001b[39;00m is_new_proxy_conn \u001b[39mand\u001b[39;00m http_tunnel_required:\n\u001b[1;32m--> 711\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\connectionpool.py:1007\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1005\u001b[0m     conn\u001b[39m.\u001b[39mtls_in_tls_required \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m-> 1007\u001b[0m conn\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    364\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000019798EB5400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\requests\\adapters.py:439\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 439\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    440\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    441\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    442\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    443\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    444\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    445\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    448\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    449\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='read.douban.com', port=443): Max retries exceeded with url: /provider/all (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000019798EB5400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\XMU\\大二下\\学科实践（二）\\项目\\python爬虫(翻页+存为xlsx) - new.ipynb 单元格 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m headers \u001b[39m=\u001b[39m { \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m'\u001b[39m: random\u001b[39m.\u001b[39mchoice(user_agents)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m url\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://read.douban.com/provider/all\u001b[39m\u001b[39m'\u001b[39m \n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m res \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url,headers\u001b[39m=\u001b[39;49mheaders,cookies\u001b[39m=\u001b[39;49mcookie,proxies\u001b[39m=\u001b[39;49mproxies)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m sp \u001b[39m=\u001b[39m BeautifulSoup(res\u001b[39m.\u001b[39mtext,\u001b[39m'\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m pb\u001b[39m=\u001b[39msp\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m,{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mprovider-group\u001b[39m\u001b[39m'\u001b[39m})\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\requests\\api.py:76\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m'\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\requests\\sessions.py:542\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    537\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    538\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[0;32m    539\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[0;32m    540\u001b[0m }\n\u001b[0;32m    541\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 542\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    544\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\requests\\sessions.py:655\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    652\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    654\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 655\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    657\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    658\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\requests\\adapters.py:510\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m RetryError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _ProxyError):\n\u001b[1;32m--> 510\u001b[0m     \u001b[39mraise\u001b[39;00m ProxyError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    513\u001b[0m     \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[1;31mProxyError\u001b[0m: HTTPSConnectionPool(host='read.douban.com', port=443): Max retries exceeded with url: /provider/all (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000019798EB5400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "#定义代理池\n",
    "proxy_list = [\n",
    "    '127.0.0.1:15732',\n",
    "    '192.168.56.1.15732'\n",
    "]\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER',\n",
    "                   'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0'\n",
    "                   ]\n",
    "proxy = random.choice(proxy_list)\n",
    "cookie={'cookie':'bid=kRRUP5Adrsc; _ga=GA1.3.1583431493.1679359048; _gid=GA1.3.240421151.1679359048; _ga=GA1.1.1583431493.1679359048; page_style=\"mobile\"; dbcl2=\"215291240:+lGgZ069L0g\"; _pk_ses.100001.a7dd=*; ck=AT7V; _ga_RXNMP372GL=GS1.1.1679406549.4.1.1679408190.60.0.0; _pk_id.100001.a7dd=0f38c905a23f4f70.1679359049.4.1679408190.1679402067.; _gat=1'}\n",
    "proxies = {\n",
    "'http':  proxy,\n",
    "'https':  proxy,\n",
    "}\n",
    "\n",
    "headers = { \n",
    "        'User-Agent': random.choice(user_agents)\n",
    "    }\n",
    "url='https://read.douban.com/provider/all' \n",
    "res = requests.get(url,headers=headers,cookies=cookie,proxies=proxies)\n",
    "sp = BeautifulSoup(res.text,'lxml') \n",
    "\n",
    "pb=sp.find('div',{'class':'provider-group'})\n",
    "#print(pb)\n",
    "\n",
    "pbs=sp.select('.provider-group')\n",
    "pbs=sp.find_all('div',{'class':'provider-group'})\n",
    "print(len(pbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='read.douban.com', port=443): Max retries exceeded with url: /provider/all (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000197B22DE490>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/aistudio/external-libraries')\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests,random\n",
    "#定义代理池\n",
    "proxy_list = [\n",
    "    '127.0.0.1:15732',\n",
    "    '192.168.56.1.15732'\n",
    "]\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER',\n",
    "                   'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0'\n",
    "                   ]\n",
    "proxy = random.choice(proxy_list)\n",
    "\n",
    "proxies = {\n",
    "'http':  proxy,\n",
    "'https':  proxy,\n",
    "}\n",
    "\n",
    "headers = { \n",
    "        'User-Agent': random.choice(user_agents)\n",
    "    }\n",
    "url='https://read.douban.com/provider/all'                        \n",
    "cookie={'cookie':'bid=kRRUP5Adrsc; _ga=GA1.3.1583431493.1679359048; _gid=GA1.3.240421151.1679359048; _ga=GA1.1.1583431493.1679359048; page_style=\"mobile\"; dbcl2=\"215291240:+lGgZ069L0g\"; _pk_ses.100001.a7dd=*; ck=AT7V; _ga_RXNMP372GL=GS1.1.1679406549.4.1.1679408190.60.0.0; _pk_id.100001.a7dd=0f38c905a23f4f70.1679359049.4.1679408190.1679402067.; _gat=1'}\n",
    "try:\n",
    "    response = requests.get(url,headers=headers,proxies=proxies)\n",
    "    #将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象, 可以传入一段字符串\n",
    "    soup = BeautifulSoup(response.text,'lxml')  \n",
    "    \n",
    "    #返回所有的<table>所有标签\n",
    "    #\n",
    "    publishes = soup.find_all('div',{'class':'provider-group'})\n",
    "    print(len(publishes))\n",
    "    pbs=[]\n",
    "    for index,pb in enumerate(publishes):\n",
    "        #if (index<=1):\n",
    "        if True:\n",
    "            pb_list={}\n",
    "            pb_list['item_name']=pb.find_next('div').text\n",
    "            item_list=[]\n",
    "            p=pb.find_next('ul')\n",
    "            li_s=p.find_all('li')\n",
    "            \n",
    "    #print(l[0])\n",
    "            for li in li_s:\n",
    "                item_li={}\n",
    "                item_li['href']='https://read.douban.com'+li.find_next('a').get('href')\n",
    "                #item_li['others']=li.find_next('div').find_next('div').find_next_sibling('div')\n",
    "                print(li.find_next('div',{'class':'name'}))\n",
    "                item_detail=li.find_next('div').find_next('div').find_next_sibling('div')\n",
    "                #print(item_detail)\n",
    "                pb_name=item_detail.find_next('div')\n",
    "                item_li['pb_name']=pb_name.text\n",
    "                #print(pb_name.text)\n",
    "                pb_count_str=pb_name.find_next_sibling('div').text\n",
    "                idx=pb_count_str.find(' ')\n",
    "                pb_count_str=pb_count_str[0:idx]\n",
    "                #print(pb_count_str)\n",
    "                item_li['pb_count']= pb_count_str\n",
    "                \n",
    "                item_list.append([pb_count_str,pb_name.text])\n",
    "                print(li.find_next('div').find_next('div').find_next_sibling('div'))  \n",
    "                #print(li.find_next('div').find_next_sibling('div'))  \n",
    "                time.sleep(0.2)\n",
    "            pb_list['item_list']=item_list\n",
    "            #print(pb_list)\n",
    "            pbs.append(pb_list)\n",
    "    #print(len(pbs))\n",
    "    #json_data = json.loads(str(pbs)) \n",
    "    json_data = json.loads(str(pbs).replace(\"\\'\",\"\\\"\").replace(\"\\\\xa0\",\"\"))  \n",
    "    with open('work/' + 'pbs.json', 'w', encoding='UTF-8') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False)\n",
    "  \n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('work/pbs.json', 'r', encoding='UTF-8') as file:\n",
    "         json_array = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='read.douban.com', port=443): Max retries exceeded with url: /provider/all (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018B6E927D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/home/aistudio/external-libraries')\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "proxy_list = [\n",
    "    '127.0.0.1:15732',\n",
    "    '192.168.56.1.15732'\n",
    "]\n",
    "proxy = random.choice(proxy_list)\n",
    "\n",
    "proxies = {\n",
    "'http':  proxy,\n",
    "'https':  proxy,\n",
    "}\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER',\n",
    "                   'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0'\n",
    "                   ]\n",
    "headers = { \n",
    "       'User-Agent': random.choice(user_agents),\n",
    "}\n",
    "url='https://read.douban.com/provider/all'                     \n",
    "cookie={'cookie':'bid=QhkSDkC-6D4; dbcl2=\"269058597:eXNEBe+CNi8\"; ll=\"108235\"; __utma=30149280.1604062177.1679555773.1679555773.1679555773.1; __utmz=30149280.1679555773.1.1.utmcsr=accounts.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __gads=ID=8a3e8dcdd0ebd524-22aef64e8edc0077:T=1679555778:RT=1679555778:S=ALNI_MYat1Lbo2a4rKijb4czFgslEoxqvw; _ga=GA1.3.1604062177.1679555773; _gid=GA1.3.939193650.1679555779; _ga=GA1.1.1604062177.1679555773; _pk_ref.100001.a7dd=%5B%22%22%2C%22%22%2C1679633561%2C%22https%3A%2F%2Fmovie.douban.com%2F%22%5D; _pk_ses.100001.a7dd=*; ck=MFsm; __gpi=UID=00000bdebf1cdfd9:T=1679555778:RT=1679633581:S=ALNI_Mb7pC52M6-drvqg3ia2C6iRFXIesA; _gat=1; _pk_id.100001.a7dd=58088c42b0c59ce5.1679555779.2.1679633623.1679555977.; _ga_RXNMP372GL=GS1.1.1679633560.3.1.1679633623.59.0.0'}\n",
    "try:\n",
    "    response = requests.get(url,headers=headers,cookies=cookie,proxies=proxies)\n",
    "    #将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象, 可以传入一段字符串\n",
    "    soup = BeautifulSoup(response.text,'lxml')  \n",
    "    \n",
    "    #返回所有的<div>所有标签\n",
    "    publishes = soup.find_all('div',{'class':'provider-group'})\n",
    "    pbs=[]\n",
    "    item_list=[]\n",
    "    #print(publishes)\n",
    "    #enumerate爬虫中的遍历\n",
    "    #遍历所有出版社\n",
    "    #pb为当前出版社\n",
    "    for index,pb in enumerate(publishes):\n",
    "        #if (index<=1):\n",
    "        if True:\n",
    "            pb_list={}\n",
    "            pb_list['item_name']=pb.find_next('div').text\n",
    "            p=pb.find_next('ul')\n",
    "            li_s=p.find_all('li')#li_s存储了当前出版社的所有数据。\n",
    "            \n",
    "        #print(li_s)\n",
    "        for li in li_s:\n",
    "            item_li={}\n",
    "            #item_li为进入当前出版社内页书单的链接\n",
    "            item_li['href']='https://read.douban.com'+li.find_next('a').get('href')\n",
    "\n",
    "            url2=item_li['href']\n",
    "            response2=requests.get(url2,headers=headers,cookies=cookie,proxies=proxies)\n",
    "            soup2 = BeautifulSoup(response2.text,'lxml')  \n",
    "            \n",
    "            #遍历当前出版社的所有书单页面\n",
    "            while soup2.find('li',class_='next')!=None:\n",
    "                #booklist为当前页面的所有<div class=info>的书籍数据\n",
    "                booklist=soup2.find_all('div',{'class':'info'})\n",
    "                #print(booklist)\n",
    "                #print(publishes2)\n",
    "                #遍历当前页面的所有书籍\n",
    "                #book为当前的书籍数据\n",
    "                for book in booklist:\n",
    "                    \n",
    "                    if(book.find('h4',class_='title')==None):\n",
    "                        continue\n",
    "                    title =book.find('h4',class_='title').text\n",
    "                    item_li['name']=title\n",
    "                    if(book.find('div',class_='sales-price')!=None):\n",
    "                        price=book.find('div',class_='sales-price').text\n",
    "                        item_li['price']=price\n",
    "                    elif(book.find('span',class_='discount-price')!=None):\n",
    "                        price=book.find('span',class_='discount-price').text\n",
    "                        item_li['price']=price\n",
    "                    elif(book.find('span',class_='price-tag')!=None):\n",
    "                        price=book.find('span',class_='price-tag').text\n",
    "                        item_li['price']=price\n",
    "                    else:\n",
    "                        continue\n",
    "                    #输出查看\n",
    "                    print(f\"《{title}》:{price}\")#1000行截断，保存成xlsx比较好\n",
    "                    #item_list用来存储要求得的书名和价格的list型数据结构，一维\n",
    "                    item_list.append([title,price])\n",
    "                    t = random.random() #随机大于0 且小于1 之间的小数\n",
    "                    time.sleep(t)\n",
    "                    \n",
    "                temp2=soup2.find('li',class_='next')\n",
    "                #若存在后页\n",
    "                if temp2.find('a')!=None:\n",
    "                    #跳转到下一页\n",
    "                    url3=url2+temp2.find('a').get('href')\n",
    "                    response2=requests.get(url3,headers=headers,cookies=cookie,proxies=proxies)\n",
    "                    soup2 = BeautifulSoup(response2.text,'lxml') \n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "    df=pd.DataFrame(item_list)\n",
    "    df.columns=['书籍名称','价格']\n",
    "    print(df)\n",
    "    #保存到excel文件中\n",
    "    df.to_excel(\"爬虫数据.xlsx\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《云边有个小卖部》:13.99元\n",
      "《病隙碎笔》:9.99元\n",
      "《你也走了很远的路吧（增订本）》:29.99元\n",
      "《亲密关系（全3册）》:48.97元\n",
      "《苏东坡传》:9.99元\n",
      "《我与地坛（插图版）》:9.99元\n",
      "《中国通史》:2.00元\n",
      "《通透》:62.00元\n",
      "《唐诗生死局（全二册）》:61.99元\n",
      "《日本蜡烛图技术（第2版）》:9.99元\n",
      "《有钱人和你想的不一样（新版）》:12.49元\n",
      "《中华帝国的衰落》:9.90元\n",
      "《紫金陈：高智商犯罪（套装全4册）》:94.99元\n",
      "《亲密关系：通往灵魂的桥梁》:16.99元\n",
      "《南渡北归·南渡（上）》:11.99元\n",
      "《山茶文具店》:10.99元\n",
      "《赛雷三分钟漫画史（套装全7册）》:127.89元\n",
      "《邻居》:40.99元\n",
      "《地球无应答》:19.98元\n",
      "《秘密》:17.99元\n",
      "《香水之书》:63.99元\n",
      "《沈氏女科：600年家传秘方大公开（套装全3册）》:55.97元\n",
      "《官路》:17.99元\n",
      "《罗马史纲》:39.99元\n",
      "《博弈论的诡计》:25.99元\n",
      "《南渡北归（套装全六册）》:69.99元\n",
      "《人间草木》:9.99元\n",
      "《人性的弱点（增订全译版）》:17.99元\n",
      "《天堂旅行团》:29.99元\n",
      "《鲜衣怒马少年时》:9.99元\n",
      "《紫金陈：推理之王系列（共3册）》:99.99元\n",
      "《古人很潮·第一季：古代名士、美男、网红等绝密档案（套装共5册）》:137.95元\n",
      "《郭论（套装全6册）》:159.75元\n",
      "《简读中国史：世界史坐标下的中国》:11.99元\n",
      "《告白》:9.90元\n",
      "《寻找百忧解》:41.99元\n",
      "《予你长生》:34.99元\n",
      "《生而为人》:39.99元\n",
      "《成何体统（全2册）》:54.99元\n",
      "《龙在宇商战小说集（套装全7册）》:94.40元\n",
      "《了不起的心理学》:29.00元\n",
      "《高智商犯罪1·设局》:20.00元\n",
      "《惩罚者（套装全3册）》:74.58元\n",
      "《让我留在你身边》:14.70元\n",
      "《大城市的兴衰》:45.49元\n",
      "《岳南作品集：一代大师们的经典重现（套装共6册）》:99.91元\n",
      "《少年前传》:48.99元\n",
      "《挣脱：一个律师的女性辩护记录》:30.79元\n",
      "《春祺夏安》:26.59元\n",
      "《九滴水·尸案调查科系列（套装全7册）》:124.99元\n",
      "《树犹如此》:17.99元\n",
      "《观山海》:17.99元\n",
      "《偷影子的人 》:9.90元\n",
      "《生活蒙太奇》:25.79元\n",
      "《倘若有柚子》:33.99元\n",
      "《你好，旧时光（全三册）》:99.99元\n",
      "《赛雷三分钟漫画三国演义·第一辑（套装共3册）》:84.97元\n",
      "《第七颗头骨》:19.99元\n",
      "《成为雅诗兰黛》:48.99元\n",
      "《如父如子》:9.90元\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\XMU\\大二下\\学科实践（二）\\项目\\python爬虫(翻页+存为xlsx) - new.ipynb 单元格 11\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#X13sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mif\u001b[39;00m temp2\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m!=\u001b[39m\u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#X13sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39m#跳转到下一页\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#X13sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     url3\u001b[39m=\u001b[39murl2\u001b[39m+\u001b[39mtemp2\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#X13sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     response2\u001b[39m=\u001b[39mrequests\u001b[39m.\u001b[39;49mget(url3,headers\u001b[39m=\u001b[39;49mheaders,cookies\u001b[39m=\u001b[39;49mcookie)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#X13sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     soup2 \u001b[39m=\u001b[39m BeautifulSoup(response2\u001b[39m.\u001b[39mtext,\u001b[39m'\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/XMU/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E5%AD%A6%E7%A7%91%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89/%E9%A1%B9%E7%9B%AE/python%E7%88%AC%E8%99%AB%28%E7%BF%BB%E9%A1%B5%2B%E5%AD%98%E4%B8%BAxlsx%29%20-%20new.ipynb#X13sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\http\\client.py:1347\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1347\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1348\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1349\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\http\\client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    308\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    309\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\http\\client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 268\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    270\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32me:\\Anaconda3\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "headers = { \n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'\n",
    "    }\n",
    "url='https://read.douban.com/provider/all'                     \n",
    "cookie={'cookie':'bid=QhkSDkC-6D4; ll=\"108235\"; __utma=30149280.1604062177.1679555773.1679555773.1679555773.1; __utmz=30149280.1679555773.1.1.utmcsr=accounts.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __gads=ID=8a3e8dcdd0ebd524-22aef64e8edc0077:T=1679555778:RT=1679555778:S=ALNI_MYat1Lbo2a4rKijb4czFgslEoxqvw; _ga=GA1.3.1604062177.1679555773; _ga=GA1.1.1604062177.1679555773; __gpi=UID=00000bdebf1cdfd9:T=1679555778:RT=1679633581:S=ALNI_Mb7pC52M6-drvqg3ia2C6iRFXIesA; apiKey=; dbcl2=\"215291240:BgbV5x3+mU4\"; ck=Jcux; _ga_RXNMP372GL=GS1.1.1695114097.4.0.1695114097.60.0.0; _gid=GA1.3.1870514339.1695114098; _pk_ref.100001.a7dd=%5B%22%22%2C%22%22%2C1695114098%2C%22https%3A%2F%2Fopen.weixin.qq.com%2F%22%5D; _pk_id.100001.a7dd=58088c42b0c59ce5.1679555779.; _pk_ses.100001.a7dd=1'}\n",
    "try:\n",
    "    response = requests.get(url,headers=headers,cookies=cookie)\n",
    "    #将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象, 可以传入一段字符串\n",
    "    soup = BeautifulSoup(response.text,'lxml')  \n",
    "    \n",
    "    #返回所有的<div>所有标签\n",
    "    publishes = soup.find_all('div',{'class':'provider-group'})\n",
    "    pbs=[]\n",
    "    item_list=[]\n",
    "    #enumerate爬虫中的遍历\n",
    "    #遍历所有出版社\n",
    "    #pb为当前出版社\n",
    "    for index,pb in enumerate(publishes):\n",
    "        #if (index<=1):\n",
    "        if True:\n",
    "            pb_list={}\n",
    "            pb_list['item_name']=pb.find_next('div').text\n",
    "            p=pb.find_next('ul')\n",
    "            li_s=p.find_all('li')#li_s存储了当前出版社的所有数据。\n",
    "            \n",
    "        #print(li_s)\n",
    "        book_name=[]\n",
    "        book_value=[]\n",
    "        for li in li_s:\n",
    "            item_li={}\n",
    "            #item_li为进入当前出版社内页书单的链接\n",
    "            item_li['href']='https://read.douban.com'+li.find_next('a').get('href')\n",
    "\n",
    "            url2=item_li['href']\n",
    "            response2=requests.get(url2,headers=headers,cookies=cookie)\n",
    "            soup2 = BeautifulSoup(response2.text,'lxml')  \n",
    "            \n",
    "            #遍历当前出版社的所有书单页面\n",
    "            while soup2.find('li',class_='next')!=None:\n",
    "                #booklist为当前页面的所有<div class=info>的书籍数据\n",
    "                booklist=soup2.find_all('div',{'class':'info'})\n",
    "                #print(publishes2)\n",
    "                #遍历当前页面的所有书籍\n",
    "                #book为当前的书籍数据\n",
    "                for book in booklist:\n",
    "                    \n",
    "                    if(book.find('h4',class_='title')==None):\n",
    "                        continue\n",
    "                    title =book.find('h4',class_='title').text\n",
    "                    item_li['name']=title\n",
    "                    if(book.find('div',class_='sales-price')!=None):\n",
    "                        price=book.find('div',class_='sales-price').text\n",
    "                        item_li['price']=price\n",
    "                    elif(book.find('span',class_='discount-price')!=None):\n",
    "                        price=book.find('span',class_='discount-price').text\n",
    "                        item_li['price']=price\n",
    "                    elif(book.find('span',class_='price-tag')!=None):\n",
    "                        price=book.find('span',class_='price-tag').text\n",
    "                        item_li['price']=price\n",
    "                    else:\n",
    "                        continue\n",
    "                    #输出查看\n",
    "                    print(f\"《{title}》:{price}\")#1000行截断，保存成xlsx比较好\n",
    "                    #item_list用来存储要求得的书名和价格的list型数据结构，一维\n",
    "                    item_list.append([title,price])\n",
    "                temp2=soup2.find('li',class_='next')\n",
    "                #若存在后页\n",
    "                if temp2.find('a')!=None:\n",
    "                    #跳转到下一页\n",
    "                    url3=url2+temp2.find('a').get('href')\n",
    "                    response2=requests.get(url3,headers=headers,cookies=cookie)\n",
    "                    soup2 = BeautifulSoup(response2.text,'lxml') \n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "    df=pd.DataFrame(item_list)\n",
    "    df.columns=['书籍名称','价格']\n",
    "    print(df)\n",
    "    #保存到excel文件中\n",
    "    df.to_excel(\"爬虫数据.xlsx\")\n",
    "\n",
    "    json_data = json.loads(str(pbs).replace(\"\\'\",\"\\\"\").replace(\"\\\\xa0\",\"\"))  \n",
    "    with open('work/' + 'pbs.json', 'w', encoding='UTF-8') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
